---
title: Insert title here
key: 635cdc359ba2623063f2538725532e05

---
## Introducing Regularization Techniques

```yaml
type: "TitleSlide"
key: "6031b2991b"
```

`@lower_third`

name: Colleen Bobbie
title: undefined


`@script`
In previous chapters, we’ve covered a brief Introduction to TensorFlow in R, learning the syntax, variables and placeholders that TensorFlow uses. We’ve been introduced to the concept of modelling in TensorFlow using a basic linear regression and we’ve built upon that knowledge to create a deep neural network, visualizing the network using TensorBoard. In this Chapter, we’ll discuss how to overcome the common problem of overfitting, in which our models perform well given any existing data, but essentially crash and burn when attempting to generalize in new situations.  
To overcome this overfitting challenge, we can apply what are known as regularization techniques. While there are several options available, in this lesson we’ll be introduced to the L2 and dropout techniques.


---
## L2 Regularization

```yaml
type: "FullSlide"
key: "1698c2806f"
```

`@part1`



`@script`



---
## Dropout 

```yaml
type: "FullSlide"
key: "7dc0b7d962"
```

`@part1`
Dropout modifies the neural network, not the cost function.
![https://assets.datacamp.com/production/repositories/4928/datasets/45a872a54b26b5a1b272c8756f692f8031c18d29/Full%20Dropout%20Diagram.JPG]()


`@script`
In opposition to the L2 technique, dropout doesn’t modify the cost function, but instead directly modifies the neural network.  For example, here is a network with 4 input variables, 6 hidden layers, and 1 output layer.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "8a0a3ca24b"
```

`@script`


