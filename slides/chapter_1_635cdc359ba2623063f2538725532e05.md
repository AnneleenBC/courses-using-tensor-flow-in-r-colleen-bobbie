---
title: Insert title here
key: 635cdc359ba2623063f2538725532e05

---
## Introducing Regularization Techniques

```yaml
type: "TitleSlide"
key: "6031b2991b"
```

`@lower_third`

name: Colleen Bobbie
title: 


`@script`
In previous chapters, we’ve covered a brief Introduction to TensorFlow in R, learning the syntax, variables and placeholders that TensorFlow uses. We’ve been introduced to the concept of modeling in TensorFlow using a basic linear regression and we’ve built upon that knowledge to create a deep neural network, visualizing the network using TensorBoard. In this Chapter, we’ll discuss one of the ways of how to overcome the common problem of overfitting, in which our models perform well given any existing data, but essentially crash and burn when attempting to generalize in new situations.  
To overcome this overfitting challenge, we can apply what are known as regularization techniques. Over the next two lessons, we'll cover the Dropout and L2 Regularization Technique.Let's dive into how the Dropout Technique works and how to implement this technique in R.


---
## Dropout

```yaml
type: "FullSlide"
key: "7dc0b7d962"
```

`@part1`
Dropout is one of the most popular forms of regularization. It modifies the neural network directly.

A full diagram may look like this:
![](https://assets.datacamp.com/production/repositories/4928/datasets/8177971b597f02d25de9b283f036940e54e78a9d/Full%20Dropout%20Diagram.JPG.jpg =40)


`@script`
The Dropout technique is one of the most popular forms of regularization when attempting to solve overfitting issues in machine learning models. To understand more about how dropout works, let's start off with a visualization of a basic network diagram with 4 input variables, 6 hidden layers, and 1 output layer.


---
## Dropout

```yaml
type: "FullSlide"
key: "228b7e5fd2"
```

`@part1`
Dropout is one of the most popular forms of regularization. It modifies the neural network directly.


The first dropped diagram may look like this:
![](https://assets.datacamp.com/production/repositories/4928/datasets/5391f32aaab3119e6ab08f39d0f62549cce74187/Dropout%201%20Diagram.JPG.jpg =40)


`@script`
In the Dropout technique, the model temporarily deletes half the hidden layers in the model (which we can now consider as the ‘dropouts’). This changes the network to something like this, where there are now only 3 hidden layers in the network.
The model runs as normal and, once the forward-propagation and back-propagation has been completed, the model is updated with the appropriate weights and biases. 

It is important to note, however, that the Input and Output layers remain unchanged in this technique.


---
## Dropout

```yaml
type: "FullSlide"
key: "57ab12b986"
```

`@part1`
Dropout is one of the most popular forms of regularization. It modifies the neural network directly.

Another diagram may look like this:
![](https://assets.datacamp.com/production/repositories/4928/datasets/b68c3a6eef1f7c72ef80d2a14f1784dce1e8e699/Dropout%202%20Diagram.JPG.jpg =40)


`@script`
This process is repeated by restoring the dropout layers, choosing a new random subset of hidden layers to delete, and updating the weights and biases in the network.  As this process is completed over and over and over again, the model is continuously updated with a new set of weights and biases that reflect the iterations we've made in the model.
But how does this help with our original overfitting problem? Dropout removes the dependence of any particular hidden layer on another, and as such, is a way of ensuring that the model will continue to perform well even if there is a loss of any individual piece of evidence when generalizing to a new situation.


---
## Dropout in R

```yaml
type: "FullSlide"
key: "831968462c"
```

`@part1`
Using the Estimators API with a dnn_classifier:

```
dnn_classifier(hidden_units = 6, 
	feature_columns = FC_list,
	n_classes = 2L, 
    dropout = 0.5)
```

Here, the dropout probability is **0.5 or 50%**, meaning that the probability of any given hidden layer will be dropped is 50%.


`@script`
Here is an example of a deep neural network classifier on which we can implement a Dropout regularization technique. In the classifier below, we have 6 hidden units, a feature columns list named 'FC_list', and 2 output classes. We can then set the dropout probability to 0.5, which means that each given hidden layer will have a 50% probability of being dropped in our model.  Note that in the Estimators API, the dropout argument by default is set to NULL, which is why we haven't yet it in our DNN model creation. 
As you can see, this regularization technique is relatively simple to implement, particularly when using the Estimators API.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "8a0a3ca24b"
```

`@script`
Now it's your turn! Let's use the Dropout Technique to implement a regularization on a deep neural network model.

