---
title: Insert title here
key: 635cdc359ba2623063f2538725532e05

---
## Introducing Regularization Techniques

```yaml
type: "TitleSlide"
key: "6031b2991b"
```

`@lower_third`

name: Colleen Bobbie
title: undefined


`@script`
In previous chapters, we’ve covered a brief Introduction to TensorFlow in R, learning the syntax, variables and placeholders that TensorFlow uses. We’ve been introduced to the concept of modelling in TensorFlow using a basic linear regression and we’ve built upon that knowledge to create a deep neural network, visualizing the network using TensorBoard. In this Chapter, we’ll discuss how to overcome the common problem of overfitting, in which our models perform well given any existing data, but essentially crash and burn when attempting to generalize in new situations.  
To overcome this overfitting challenge, we can apply what are known as regularization techniques. Over the next two lessons, we'll cover the Dropout and L2 Regularization Technique.


---
## Dropout 

```yaml
type: "FullSlide"
key: "7dc0b7d962"
```

`@part1`
Dropout is one of the most popular forms of regularization. It modifies the neural network directly.

A full diagram may look like this:
![](https://assets.datacamp.com/production/repositories/4928/datasets/8177971b597f02d25de9b283f036940e54e78a9d/Full%20Dropout%20Diagram.JPG.jpg =40)


`@script`
Dropout is one of the most popular forms of regularization when attempting to solve overfitting issues in machine learning models. To understand more about how dropout works, here is a basic network diagram with 4 input variables, 6 hidden layers, and 1 output layer.


---
## Dropout

```yaml
type: "FullSlide"
key: "228b7e5fd2"
```

`@part1`
Dropout is one of the most popular forms of regularization. It modifies the neural network directly.


The first dropped diagram may look like this:
![](https://assets.datacamp.com/production/repositories/4928/datasets/5391f32aaab3119e6ab08f39d0f62549cce74187/Dropout%201%20Diagram.JPG.jpg =40)


`@script`
In the Dropout technique, the model temporarily deletes half the hidden layers in the model (also known as the ‘dropouts’). This changes the network to something like this, where there are now only 3 hidden layers in the network.
Once the forward-propagation and back-propagation has been completed, the model is updated with the appropriate weights and biases. 

It is important to note that the Input and Output layers remain unchanged in this technique.


---
## Dropout

```yaml
type: "FullSlide"
key: "57ab12b986"
```

`@part1`
Dropout is one of the most popular forms of regularization. It modifies the neural network directly.

Another diagram may look like this:
![](https://assets.datacamp.com/production/repositories/4928/datasets/b68c3a6eef1f7c72ef80d2a14f1784dce1e8e699/Dropout%202%20Diagram.JPG.jpg =40)


`@script`
This process is repeated by restoring the dropout layers, choosing a new random subset of hidden layers to delete, and updating the weights and biases in the network.  As this process is completed over and over again, the model is updated with a new set of weights and biases. 
But how does this help with our original overfitting problem? Dropout removes the dependence of any particular hidden layer on another, and as such, is a way of ensuring that the model will continue to perform well even if there is a loss of any individual piece of evidence when generalizing to a new situation.


---
## Dropout in R

```yaml
type: "FullSlide"
key: "831968462c"
```

`@part1`
Using the Estimators API with a dnn_classifier:

```
dnn_classifier(hidden_units = 6, 
	feature_columns = FC_list,
	n_classes = 2L, 
    dropout = 0.5)
```

Here, the dropout probability is **0.5 or 50%**, meaning that the probability of any given hidden layer will be dropped is 50%.


`@script`
Here is an example of a deep neural network classifier on which we can implement a Dropout regularization technique. Note that in the Estimators API, the dropout argument by default is set to NULL, which is why we haven't seen it yet in our DNN model creation. In a basic model such as this, we can implement a dropout probability of 50% on a network that has 6 hidden units, a features column list named 'FC_List', and 2 output classes.

As you can see, this regularization technique is relatively simple to implement when using the Estimators API.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "8a0a3ca24b"
```

`@script`
Now it's your turn! Let's use the Dropout Technique to implement a regularization on a deep neural network model.

